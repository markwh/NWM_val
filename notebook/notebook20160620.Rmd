---
title: "notebook20160620"
author: "Mark Hagemann"
date: "June 20, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today I'll look at a simulation output file.

First, here's the forecast file I looked at yesterday.

```{r}
nc1 <- RNetCDF::open.nc("~/Downloads/nwm.t00z.short_range.channel_rt.f004.conus.nc_georeferenced.nc")

nc2 <- read.nc(nc1)

str(nc2, 1)
```

OK, this apparently doesn't preserve the date for which it was created. Good to know. 

For now I'll get the simulation file for channels from 20160618t00z. I think that may have been what I have for the short-term forecast.

```{r}
assim1 <- open.nc("~/Downloads/nwm.20160618.t00z.analysis_assim.channel_rt.tm00.conus.nc_georeferenced.nc")
assim2 <- read.nc(assim1)

str(assim2, 1)
```

Appears to have same structure as forecast output. Not clear how data plays into this. 

Are all 2.7M reaches acounted for? Yes; that's the length of the vector for each product.

### Getting USGS gage data to compare

I found USGS stream gages indexed to NHDPlus locations [here](https://www.sciencebase.gov/catalog/item/555a2659e4b0a92fa7ea0fac).

It appears I can extract this info from the .dbf file in the shapefile folder.

- ***Convert to csv first!***

```{r}
gageinfo <- read.csv("data/Gage_Loc_052015.csv", colClasses = "character") %>% 
  transmute(reach = REACHCODE.C.14, stationId = SOURCE_FEA.C.40)
pryr::object_size(gageinfo) # 4 MB, or 9.9MB when coerced to character
head(gageinfo)
str(gageinfo)
```

So now I have a mapping of streamgages to NHDPlus reaches. But I'd rather not have to request data using dataRetrieval every hour. 

- AHA! It's available from the tethys app. See "USGS timeslices"
    - in 15-min resolution!
- Getting 20160618 00:00:00 timeslice.

```{r}
usgs1 <- open.nc("data/ncdf/2016-06-18_00-00-00.15min.usgsTimeSlice.ncdf")
usgs2 <- read.nc(usgs1)
pryr::object_size(usgs2) # Pretty small (0.5 MB)

str(usgs2, 1)

usgs3 <- usgs2 %>% 
  lapply(as.vector) %>% 
  data.frame() %>% 
  mutate(stationId = trimws(stationId),
         time = ymd_hms(time))

str(usgs3)
```

Also moving all netcdf data to data/ncdf/ folder.

Here's how it will look in munge/ script.

```{r}
st1 <- open.nc("data/ncdf/nwm.t00z.short_range.channel_rt.f004.conus.nc_georeferenced.nc")
st2 <- read.nc(st1)
st3 <- st2[-1] %>% 
  lapply(as.vector) %>% 
  data.frame() %>% 
  mutate(reach = as.character(station_id))
```

todo$add("find out whether USGS times need to be adjusted")

Potential problem: the gage mapping I have is from 2015; new gages may have come online since then. 

- Ignore this for now. Later I can make a new mapping object. 

todo$add("Join to NHDPlus reaches.")

```{r}
glimpse(usgs3)
glimpse(gageinfo)
usgs4 <- usgs3 %>% 
  left_join(gageinfo, by = "stationId")

glimpse(usgs4)
glimpse(st3)

summary(as.numeric(usgs4$reach)) #689 are missing reach info. 
```

Unclear how reach number relates to stationId.

```{r}
summary(st3$station_id)
head(st3$station_id)

summary(as.numeric())
head(usgs4$reach)
```

Redoing with new key file. Obtained from [here](http://www.horizon-systems.com/NHDPlus/V2NationalData.php). (NHDPlusV21_NationalData_GageLoc_05.7z file)

```{r}
gageinfo <- read.csv("data/gageloc/GageLoc.csv")
glimpse(gageinfo)

gageinfo <- read.csv("data/gageloc/GageLoc.csv", colClasses = "character") %>% 
  transmute(reach = FLComID.N.9.0, stationId = SOURCE_FEA.C.40)

usgs4 <- usgs3 %>% 
  left_join(gageinfo, by = "stationId")

sapply(usgs4, function(x) sum(is.na(x))) 
```

So 580 gages missing COMIDs. 
todo$add("Investigate 580 missing comIDs after joingin usgs to reaches")

Now join usgs to nwm forecasts.

```{r}
glimpse(usgs4)
glimpse(st3)
dat1 <- usgs4 %>% 
  left_join(st3, by = "reach")

glimpse(dat1)
cache(dat1)
```

Amost time to give this project a rest for now. Before I do, sketch out next steps. 

todo$add("aggregate USGS snapshots to time-resolution of forecast products")
todo$add("Scrutinize Arroyo Colorado comparison")
todo$add("Write functions to streamline data import and preprocessing for forecast products")
todo$add("Write functions to calculate comparison statistics")

## Afternoon session

Back after a lunch break. Looking again at iRods data. This is where real-time NWM output can be found. 

- How to change zone?
    - OK, just simple matter of using icd to proper location. In this case I did `icd /nwmZone/home/nwm/data`
- I should probably use their REST API eventually. todo$add("grok iRODS REST API")

### Checking out Arroyo Colorado data

Idea is to start with 2-3 stations, get stats for these first. 

Still need to get lots of datasets. Should automate this. Probaby via api. 

1. get 2 datasets (usgs and an arbitrary forecast)
2. subset to just the locations I'm interested in.
3. rm all but the little piece 

